{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a1e710db",
      "metadata": {
        "id": "a1e710db",
        "outputId": "f70d4c05-d26b-46ba-e2db-7f709ccc9acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Parameters\n",
        "IMG_SIZE = (128, 128)     # Resize frames to a fixed resolution\n",
        "NUM_INTERPOLATED = 1      # Number of frames to interpolate between the start and end\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "def extract_frames(video_path):\n",
        "    \"\"\"Extracts all frames from a given video file.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def save_sequence(sequence, output_dir, video_name, seq_index):\n",
        "    \"\"\"Saves a sequence of frames into a dedicated subfolder.\"\"\"\n",
        "    seq_folder = os.path.join(output_dir, f\"{video_name}_seq_{seq_index}\")\n",
        "    os.makedirs(seq_folder, exist_ok=True)\n",
        "    for i, frame in enumerate(sequence):\n",
        "        frame_path = os.path.join(seq_folder, f\"frame_{i}.jpg\")\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "\n",
        "def generate_dataset(input_folder, train_folder, test_folder, num_interpolated=NUM_INTERPOLATED, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Processes each video in the input folder.\n",
        "\n",
        "    For each video, a sliding window of length (num_interpolated + 2) is used\n",
        "    to generate sequences where the first and last frames are the inputs for interpolation,\n",
        "    and the frames in between are used as ground truth.\n",
        "\n",
        "    Each sequence is randomly assigned to train or test.\n",
        "    \"\"\"\n",
        "    # The total sequence length includes the starting and ending frames\n",
        "    sequence_length = num_interpolated + 2\n",
        "\n",
        "    os.makedirs(train_folder, exist_ok=True)\n",
        "    os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            video_path = os.path.join(input_folder, filename)\n",
        "            video_name = os.path.splitext(filename)[0]\n",
        "            frames = extract_frames(video_path)\n",
        "            total_frames = len(frames)\n",
        "            seq_index = 0\n",
        "            # Slide a window over the frames to generate sequences\n",
        "            for i in range(total_frames - sequence_length + 1):\n",
        "                sequence = frames[i:i + sequence_length]\n",
        "                # Randomly assign the sequence to training or testing set\n",
        "                if random.random() < train_ratio:\n",
        "                    save_sequence(sequence, train_folder, video_name, seq_index)\n",
        "                else:\n",
        "                    save_sequence(sequence, test_folder, video_name, seq_index)\n",
        "                seq_index += 1\n",
        "\n",
        "\n",
        "input_folder = \"input\"    # Folder containing your mp4 videos\n",
        "train_folder = \"train\"    # Output folder for training sequences\n",
        "test_folder = \"test\"      # Output folder for testing sequences\n",
        "generate_dataset(input_folder, train_folder, test_folder)"
      ],
      "metadata": {
        "id": "29FD6OxNoALV"
      },
      "id": "29FD6OxNoALV",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Now define 'dataloader' instance considering previous code.\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class FrameInterpolationDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.sequences = []\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            if len(files) > 0 and all(f.endswith('.jpg') for f in files):\n",
        "                self.sequences.append(subdir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_dir = self.sequences[idx]\n",
        "        frames = []\n",
        "        for i in range(len(os.listdir(sequence_dir))):  # Iterate over all frames\n",
        "          frame_path = os.path.join(sequence_dir, f\"frame_{i}.jpg\")\n",
        "          frame = cv2.imread(frame_path)\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert BGR to RGB\n",
        "\n",
        "          if self.transform:\n",
        "              frame = self.transform(image=frame)['image']\n",
        "          frames.append(frame)\n",
        "\n",
        "        frames = np.stack(frames) # Stack to create a tensor\n",
        "        frames = torch.tensor(frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0 # Normalize\n",
        "        return frames\n",
        "\n",
        "\n",
        "# Example usage (assuming you have 'train' and 'test' folders)\n",
        "train_dataset = FrameInterpolationDataset(root_dir='train')\n",
        "test_dataset = FrameInterpolationDataset(root_dir='test')\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Use the slider value 'foo'\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Use the slider value 'foo'\n"
      ],
      "metadata": {
        "id": "6929fuvjoU5Y"
      },
      "id": "6929fuvjoU5Y",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4f07dc",
      "metadata": {
        "id": "2b4f07dc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class S6Layer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(S6Layer, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, input_dim)\n",
        "        h = self.activation(self.linear1(x))\n",
        "        out = self.linear2(h)\n",
        "        return out + x  # Residual connection\n",
        "\n",
        "class VFIMamba(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(VFIMamba, self).__init__()\n",
        "        self.s6_layers = nn.ModuleList([S6Layer(input_dim, hidden_dim) for _ in range(num_layers)])\n",
        "        self.output_layer = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.s6_layers:\n",
        "            x = layer(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = 128 * 128 * 3  # Assuming 128x128 RGB images\n",
        "hidden_dim = 512\n",
        "num_layers = 4\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = VFIMamba(input_dim, hidden_dim, num_layers)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in train_dataloader:  # Define 'dataloader' to load your dataset\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54f249ba",
      "metadata": {
        "id": "54f249ba"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_dataloader:  # Define 'test_dataloader' for your test set\n",
        "        outputs = model(inputs)\n",
        "        # Reshape and convert outputs to images\n",
        "        output_images = outputs.view(-1, 128, 128, 3).cpu().numpy()\n",
        "        target_images = targets.view(-1, 128, 128, 3).cpu().numpy()\n",
        "        # Compare output_images with target_images\n",
        "        for i, (output_img, target_img) in enumerate(zip(output_images, target_images)):\n",
        "            combined = np.hstack((output_img, target_img))\n",
        "            cv2.imshow(f'Output vs Target {i}', combined)\n",
        "            cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whole code"
      ],
      "metadata": {
        "id": "yE4XDDA5rqX2"
      },
      "id": "yE4XDDA5rqX2"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "from torchvision import transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import logging\n",
        "import yaml\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for hyperparameters\"\"\"\n",
        "    img_size: Tuple[int, int] = (128, 128)\n",
        "    num_interpolated: int = 1\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 10\n",
        "    input_dim: int = 128 * 128 * 3\n",
        "    hidden_dim: int = 512\n",
        "    num_layers: int = 4\n",
        "    learning_rate: float = 1e-4\n",
        "    train_ratio: float = 0.8\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class FrameProcessor:\n",
        "    \"\"\"Handles video frame extraction and processing\"\"\"\n",
        "    def __init__(self, img_size: Tuple[int, int]):\n",
        "        self.img_size = img_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def extract_frames(self, video_path: str) -> List[np.ndarray]:\n",
        "        \"\"\"Extracts and processes frames from video\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        try:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, self.img_size)\n",
        "                frames.append(frame)\n",
        "        finally:\n",
        "            cap.release()\n",
        "        return frames\n",
        "\n",
        "    def save_sequence(self, sequence: List[np.ndarray],\n",
        "                     output_dir: Path, video_name: str,\n",
        "                     seq_index: int) -> None:\n",
        "        \"\"\"Saves frame sequence to disk\"\"\"\n",
        "        seq_folder = output_dir / f\"{video_name}_seq_{seq_index}\"\n",
        "        seq_folder.mkdir(exist_ok=True, parents=True)\n",
        "        for i, frame in enumerate(sequence):\n",
        "            frame_path = seq_folder / f\"frame_{i}.jpg\"\n",
        "            cv2.imwrite(str(frame_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "class FrameInterpolationDataset(Dataset):\n",
        "    \"\"\"Dataset class for frame sequences\"\"\"\n",
        "    def __init__(self, root_dir: str, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.sequences = list(self.root_dir.glob(\"**/frame_0.jpg\"))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        sequence_dir = self.sequences[idx].parent\n",
        "        frames = []\n",
        "        for frame_path in sorted(sequence_dir.glob(\"*.jpg\")):\n",
        "            frame = cv2.imread(str(frame_path))\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "            frames.append(frame)\n",
        "        return torch.stack(frames)\n",
        "\n",
        "class S6Layer(nn.Module):\n",
        "    \"\"\"Single S6 layer implementation\"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(input_dim)\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x + residual\n",
        "\n",
        "class VFIMamba(nn.Module):\n",
        "    \"\"\"Main model architecture\"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(config.input_dim, config.hidden_dim)\n",
        "        self.s6_layers = nn.ModuleList([\n",
        "            S6Layer(config.hidden_dim, config.hidden_dim)\n",
        "            for _ in range(config.num_layers)\n",
        "        ])\n",
        "        self.output_projection = nn.Linear(config.hidden_dim, config.input_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B, T, -1)  # Flatten spatial dimensions\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        for layer in self.s6_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.output_projection(x)\n",
        "        return x.view(B, T, C, H, W)\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Handles model training and evaluation\"\"\"\n",
        "    def __init__(self, model: nn.Module, config: Config):\n",
        "        self.model = model.to(config.device)\n",
        "        self.config = config\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader) -> float:\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(self.config.device)\n",
        "            start_frames = batch[:, 0]\n",
        "            end_frames = batch[:, -1]\n",
        "            target_frames = batch[:, 1:-1]\n",
        "\n",
        "            with autocast():\n",
        "                pred_frames = self.model(torch.cat([start_frames, end_frames], dim=1))\n",
        "                loss = self.criterion(pred_frames, target_frames)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "def main():\n",
        "    # Load configuration\n",
        "    config = Config()\n",
        "\n",
        "    # Initialize components\n",
        "    processor = FrameProcessor(config.img_size)\n",
        "    model = VFIMamba(config)\n",
        "    trainer = Trainer(model, config)\n",
        "\n",
        "    # Setup datasets\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = FrameInterpolationDataset(\"train\", transform=transform)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(\"Starting training...\")\n",
        "    for epoch in range(config.epochs):\n",
        "        loss = trainer.train_epoch(train_dataloader)\n",
        "        logger.info(f\"Epoch [{epoch+1}/{config.epochs}] Loss: {loss:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), \"frame_interpolation_model.pth\")\n",
        "    logger.info(\"Training completed. Model saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "X_Z3oeBWrxA8",
        "outputId": "7a2cd098-f8b5-4e8d-d7c3-572dc7b65093",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X_Z3oeBWrxA8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a2989bdcfec5>:146: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "source": [
        "class VFIMamba(nn.Module):\n",
        "    \"\"\"Main model architecture\"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(config.input_dim, config.hidden_dim)\n",
        "        self.s6_layers = nn.ModuleList([\n",
        "            S6Layer(config.hidden_dim, config.hidden_dim)\n",
        "            for _ in range(config.num_layers)\n",
        "        ])\n",
        "        self.output_projection = nn.Linear(config.hidden_dim, config.input_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Original: B, T, C, H, W = x.shape\n",
        "        # The input tensor has shape (batch_size, channels, height, width)\n",
        "        # We need to add a dimension for the sequence length (T)\n",
        "        # Assuming the input contains start and end frames, T = 2\n",
        "        x = x.unsqueeze(1)  # Add a dimension for sequence length\n",
        "\n",
        "        B, T, C, H, W = x.shape # Now x has the expected 5 dimensions\n",
        "        x = x.view(B, T, -1)  # Flatten spatial dimensions\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        for layer in self.s6_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.output_projection(x)\n",
        "        return x.view(B, T, C, H, W)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1nWLUmmutfsW"
      },
      "id": "1nWLUmmutfsW",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def test_interpolation(model_path: str, video_path: str, output_path: str, config: Config):\n",
        "    # Load model\n",
        "    model = VFIMamba(config)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    model.to(config.device)\n",
        "\n",
        "    # Process video\n",
        "    processor = FrameProcessor(config.img_size)\n",
        "    frames = processor.extract_frames(video_path)\n",
        "\n",
        "    # Create output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 30.0, config.img_size)\n",
        "\n",
        "    # Process frames in pairs\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(frames) - 1):\n",
        "            frame1 = torch.from_numpy(frames[i]).unsqueeze(0).to(config.device)\n",
        "            frame2 = torch.from_numpy(frames[i + 1]).unsqueeze(0).to(config.device)\n",
        "\n",
        "            # Generate interpolated frame\n",
        "            interpolated = model(torch.stack([frame1, frame2], dim=1))\n",
        "\n",
        "            # Write frames to video\n",
        "            out.write(cv2.cvtColor(frames[i], cv2.COLOR_RGB2BGR))\n",
        "            out.write(cv2.cvtColor(\n",
        "                interpolated.cpu().numpy()[0],\n",
        "                cv2.COLOR_RGB2BGR\n",
        "            ))\n",
        "\n",
        "        # Write final frame\n",
        "        out.write(cv2.cvtColor(frames[-1], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    out.release()\n",
        "\n",
        "# Test the model\n",
        "test_interpolation(\n",
        "    model_path=\"frame_interpolation_model.pth\",\n",
        "    video_path=\"input/motion.mp4\",\n",
        "    output_path=\"output/interpolated.mp4\",\n",
        "    config=Config()\n",
        ")"
      ],
      "metadata": {
        "id": "Dl69nup3rvIv"
      },
      "id": "Dl69nup3rvIv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}