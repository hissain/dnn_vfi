{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "from torchvision import transforms\n",
        "from torch.amp import autocast, GradScaler\n",
        "import logging\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "import torch.utils.checkpoint\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class with memory-optimized settings\"\"\"\n",
        "    img_size: Tuple[int, int] = (128, 128)\n",
        "    num_interpolated: int = 1\n",
        "    batch_size: int = 8  # Reduced batch size\n",
        "    epochs: int = 10\n",
        "    hidden_dim: int = 256  # Reduced hidden dimension\n",
        "    num_layers: int = 4\n",
        "    learning_rate: float = 1e-4\n",
        "    train_ratio: float = 0.8\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    @property\n",
        "    def input_dim(self) -> int:\n",
        "        return self.img_size[0] * self.img_size[1] * 3\n",
        "\n",
        "class FrameProcessor:\n",
        "    \"\"\"Handles video frame extraction and processing\"\"\"\n",
        "    def __init__(self, img_size: Tuple[int, int]):\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def process_frame(self, frame: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Process a single frame\"\"\"\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, self.img_size)\n",
        "        return frame\n",
        "\n",
        "    def extract_frames(self, video_path: str) -> List[np.ndarray]:\n",
        "        \"\"\"Extracts and processes frames from video\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        try:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = self.process_frame(frame)\n",
        "                frames.append(frame)\n",
        "        finally:\n",
        "            cap.release()\n",
        "        return frames\n",
        "\n",
        "class S6Layer(nn.Module):\n",
        "    \"\"\"Updated S6 layer with consistent dtype\"\"\"\n",
        "    def __init__(self, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        dtype = torch.float32\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim).to(dtype)\n",
        "        self.linear1 = nn.Linear(hidden_dim, hidden_dim * 4).to(dtype)\n",
        "        self.linear2 = nn.Linear(hidden_dim * 4, hidden_dim).to(dtype)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x + residual\n",
        "\n",
        "class VFIMamba(nn.Module):\n",
        "    \"\"\"AMP-compatible model architecture\"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Ensure all layers use the same dtype\n",
        "        dtype = torch.float32\n",
        "\n",
        "        # Encoder\n",
        "        self.conv_encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1).to(dtype),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1).to(dtype),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, config.hidden_dim, kernel_size=3, padding=1).to(dtype)\n",
        "        )\n",
        "\n",
        "        # Reduced dimension for S6 layers\n",
        "        reduced_dim = config.hidden_dim // 2\n",
        "        self.s6_layers = nn.ModuleList([\n",
        "            S6Layer(reduced_dim).to(dtype)\n",
        "            for _ in range(config.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(config.hidden_dim, 64, kernel_size=4, stride=2, padding=1).to(dtype),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1).to(dtype),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1).to(dtype),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Projection layers\n",
        "        self.dim_reduce = nn.Conv2d(config.hidden_dim, reduced_dim, 1).to(dtype)\n",
        "        self.dim_expand = nn.Conv2d(reduced_dim, config.hidden_dim, 1).to(dtype)\n",
        "\n",
        "    @torch.cuda.amp.autocast()\n",
        "    def process_sequence(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Process a sequence through S6 layers with gradient checkpointing\"\"\"\n",
        "        for layer in self.s6_layers:\n",
        "            x = torch.utils.checkpoint.checkpoint(layer, x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # Ensure input is float32\n",
        "        x = x.to(torch.float32)\n",
        "\n",
        "        # Process frames in chunks\n",
        "        encoded_frames = []\n",
        "        chunk_size = 2\n",
        "\n",
        "        for i in range(0, T, chunk_size):\n",
        "            chunk = x[:, i:i+chunk_size].reshape(-1, C, H, W)\n",
        "            encoded = self.conv_encoder(chunk)\n",
        "            encoded_frames.append(encoded)\n",
        "\n",
        "        x = torch.cat(encoded_frames, dim=0)\n",
        "        _, C_hidden, H_hidden, W_hidden = x.shape\n",
        "\n",
        "        # Reduce dimensions\n",
        "        x = self.dim_reduce(x)\n",
        "        x = x.view(B, T, -1, H_hidden, W_hidden)\n",
        "\n",
        "        # Process spatial chunks\n",
        "        spatial_chunks = []\n",
        "        chunk_size = H_hidden // 4\n",
        "\n",
        "        for i in range(0, H_hidden, chunk_size):\n",
        "            chunk = x[..., i:i+chunk_size, :]\n",
        "            chunk = chunk.reshape(B * chunk_size * W_hidden, T, -1)\n",
        "            chunk = self.process_sequence(chunk)\n",
        "            spatial_chunks.append(chunk)\n",
        "\n",
        "        x = torch.cat(spatial_chunks, dim=0)\n",
        "        x = x.view(B, H_hidden, W_hidden, T, -1)\n",
        "        x = x.permute(0, 3, 4, 1, 2)\n",
        "\n",
        "        # Expand dimensions\n",
        "        x = x.reshape(B * T, -1, H_hidden, W_hidden)\n",
        "        x = self.dim_expand(x)\n",
        "\n",
        "        # Decode frames\n",
        "        decoded_frames = []\n",
        "        chunk_size = 2\n",
        "\n",
        "        for i in range(0, B * T, chunk_size):\n",
        "            chunk = x[i:i+chunk_size]\n",
        "            decoded = self.conv_decoder(chunk)\n",
        "            decoded_frames.append(decoded)\n",
        "\n",
        "        x = torch.cat(decoded_frames, dim=0)\n",
        "        x = x.view(B, T, C, H, W)\n",
        "\n",
        "        return x\n",
        "\n",
        "class FrameInterpolationDataset(Dataset):\n",
        "    \"\"\"Dataset class for frame sequences\"\"\"\n",
        "    def __init__(self, root_dir: str, img_size: Tuple[int, int], transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "        self.sequences = list(self.root_dir.glob(\"**/frame_0.jpg\"))\n",
        "\n",
        "        if len(self.sequences) == 0:\n",
        "            raise RuntimeError(f\"No sequences found in {root_dir}\")\n",
        "\n",
        "        logger.info(f\"Found {len(self.sequences)} sequences in {root_dir}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        sequence_dir = self.sequences[idx].parent\n",
        "        frames = []\n",
        "\n",
        "        frame_paths = sorted(sequence_dir.glob(\"*.jpg\"),\n",
        "                           key=lambda x: int(x.stem.split('_')[1]))\n",
        "\n",
        "        for frame_path in frame_paths:\n",
        "            frame = cv2.imread(str(frame_path))\n",
        "            if frame is None:\n",
        "                raise RuntimeError(f\"Failed to load image: {frame_path}\")\n",
        "\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, self.img_size)\n",
        "\n",
        "            frame = torch.from_numpy(frame).float() / 255.0\n",
        "            frame = frame.permute(2, 0, 1)\n",
        "\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "\n",
        "            frames.append(frame)\n",
        "\n",
        "        frames_tensor = torch.stack(frames)\n",
        "\n",
        "        expected_shape = (len(frame_paths), 3, self.img_size[1], self.img_size[0])\n",
        "        if frames_tensor.shape != expected_shape:\n",
        "            raise RuntimeError(\n",
        "                f\"Incorrect tensor shape. Expected {expected_shape}, got {frames_tensor.shape}\"\n",
        "            )\n",
        "\n",
        "        return frames_tensor\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Updated trainer with proper AMP handling\"\"\"\n",
        "    def __init__(self, model: nn.Module, config: Config):\n",
        "        self.model = model.to(config.device)\n",
        "        self.config = config\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader) -> float:\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            # Ensure input data is float32\n",
        "            batch = batch.to(self.config.device, dtype=torch.float32)\n",
        "\n",
        "            start_frames = batch[:, [0]]\n",
        "            end_frames = batch[:, [-1]]\n",
        "            target_frames = batch[:, 1:-1]\n",
        "\n",
        "            input_frames = torch.cat([start_frames, end_frames], dim=1)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                pred_frames = self.model(input_frames)\n",
        "                loss = self.criterion(pred_frames, target_frames)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "def generate_dataset(input_folder: str, train_folder: str, test_folder: str, config: Config):\n",
        "    \"\"\"Generate dataset from input videos\"\"\"\n",
        "    processor = FrameProcessor(config.img_size)\n",
        "\n",
        "    for folder in [train_folder, test_folder]:\n",
        "        Path(folder).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    video_files = list(Path(input_folder).glob(\"*.mp4\"))\n",
        "    if not video_files:\n",
        "        raise RuntimeError(f\"No MP4 files found in {input_folder}\")\n",
        "\n",
        "    for video_file in video_files:\n",
        "        logger.info(f\"Processing video: {video_file}\")\n",
        "\n",
        "        frames = processor.extract_frames(str(video_file))\n",
        "        sequence_length = config.num_interpolated + 2\n",
        "\n",
        "        if len(frames) < sequence_length:\n",
        "            logger.warning(f\"Video {video_file} too short, skipping\")\n",
        "            continue\n",
        "\n",
        "        num_sequences = len(frames) - sequence_length + 1\n",
        "        for i in range(num_sequences):\n",
        "            sequence = frames[i:i + sequence_length]\n",
        "\n",
        "            output_dir = Path(train_folder if random.random() < config.train_ratio else test_folder)\n",
        "            seq_folder = output_dir / f\"{video_file.stem}_seq_{i}\"\n",
        "            seq_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "            for j, frame in enumerate(sequence):\n",
        "                frame_path = seq_folder / f\"frame_{j}.jpg\"\n",
        "                cv2.imwrite(str(frame_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        logger.info(f\"Generated {num_sequences} sequences from {video_file}\")\n",
        "\n",
        "def test_interpolation(model_path: str, video_path: str, output_path: str, config: Config):\n",
        "    \"\"\"Test the model on a video\"\"\"\n",
        "    model = VFIMamba(config)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    model.to(config.device)\n",
        "\n",
        "    processor = FrameProcessor(config.img_size)\n",
        "    frames = processor.extract_frames(video_path)\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 60.0, config.img_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(frames) - 1):\n",
        "            frame1 = torch.from_numpy(frames[i]).permute(2, 0, 1).float() / 255.0\n",
        "            frame2 = torch.from_numpy(frames[i + 1]).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "            input_frames = torch.stack([frame1, frame2]).unsqueeze(0).to(config.device)\n",
        "\n",
        "            interpolated = model(input_frames)\n",
        "\n",
        "            frame1_np = (frame1.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "            interpolated_np = (interpolated[0, 0].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "\n",
        "            out.write(cv2.cvtColor(frame1_np, cv2.COLOR_RGB2BGR))\n",
        "            out.write(cv2.cvtColor(interpolated_np, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        final_frame = (frames[-1] * 255).astype(np.uint8)\n",
        "        out.write(cv2.cvtColor(final_frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    out.release()\n",
        "    logger.info(f\"Interpolated video saved to {output_path}\")\n",
        "\n",
        "def main():\n",
        "    # Initialize configuration\n",
        "    config = Config()\n",
        "    logger.info(f\"Using device: {config.device}\")\n",
        "\n",
        "    # Clean existing datasets\n",
        "    for folder in ['train', 'test']:\n",
        "        if Path(folder).exists():\n",
        "            shutil.rmtree(folder)\n",
        "\n",
        "    # Generate dataset\n",
        "    generate_dataset(\"input\", \"train\", \"test\", config)\n",
        "\n",
        "    # Initialize model and trainer\n",
        "    model = VFIMamba(config)\n",
        "    trainer = Trainer(model, config)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = FrameInterpolationDataset(\n",
        "        \"train\",\n",
        "        img_size=config.img_size,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0 if config.device == \"cpu\" else 4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(\"Starting training...\")\n",
        "    for epoch in range(config.epochs):\n",
        "        loss = trainer.train_epoch(train_dataloader)\n",
        "        logger.info(f\"Epoch [{epoch+1}/{config.epochs}] Loss: {loss:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), \"frame_interpolation_model.pth\")\n",
        "    logger.info(\"Training completed. Model saved.\")\n",
        "\n",
        "    # Test model on a video\n",
        "    if list(Path(\"input\").glob(\"*.mp4\")):\n",
        "        test_video = next(Path(\"input\").glob(\"*.mp4\"))\n",
        "        test_interpolation(\n",
        "            \"frame_interpolation_model.pth\",\n",
        "            str(test_video),\n",
        "            \"output_interpolated.mp4\",\n",
        "            config\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfgyo7IZzBXu",
        "outputId": "29b422ae-10e1-4a78-d1d2-220f231be629"
      },
      "id": "xfgyo7IZzBXu",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d69b2519d22e>:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @torch.cuda.amp.autocast()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-1-d69b2519d22e>:264: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([8, 1, 3, 128, 128])) that is different to the input size (torch.Size([8, 2, 3, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([6, 1, 3, 128, 128])) that is different to the input size (torch.Size([6, 2, 3, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "<ipython-input-1-d69b2519d22e>:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "from torchvision import transforms\n",
        "from torch.amp import autocast, GradScaler\n",
        "import logging\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "import torch.utils.checkpoint\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class with memory-optimized settings\"\"\"\n",
        "    img_size: Tuple[int, int] = (128, 128)\n",
        "    num_interpolated: int = 1\n",
        "    batch_size: int = 8\n",
        "    epochs: int = 10\n",
        "    hidden_dim: int = 256\n",
        "    num_layers: int = 4\n",
        "    learning_rate: float = 1e-4\n",
        "    train_ratio: float = 0.8\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    @property\n",
        "    def input_dim(self) -> int:\n",
        "        return self.img_size[0] * self.img_size[1] * 3\n",
        "\n",
        "# [Previous FrameProcessor class remains the same]\n",
        "\n",
        "class S6Layer(nn.Module):\n",
        "    \"\"\"Updated S6 layer with consistent dtype\"\"\"\n",
        "    def __init__(self, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.linear1 = nn.Linear(hidden_dim, hidden_dim * 4)\n",
        "        self.linear2 = nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x + residual\n",
        "\n",
        "class VFIMamba(nn.Module):\n",
        "    \"\"\"AMP-compatible model architecture\"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Encoder\n",
        "        self.conv_encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, config.hidden_dim, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        # Reduced dimension for S6 layers\n",
        "        reduced_dim = config.hidden_dim // 2\n",
        "        self.s6_layers = nn.ModuleList([\n",
        "            S6Layer(reduced_dim)\n",
        "            for _ in range(config.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(config.hidden_dim, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Projection layers\n",
        "        self.dim_reduce = nn.Conv2d(config.hidden_dim, reduced_dim, 1)\n",
        "        self.dim_expand = nn.Conv2d(reduced_dim, config.hidden_dim, 1)\n",
        "\n",
        "    # Removed @torch.utils.checkpoint.checkpoint\n",
        "    def process_sequence(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Process a sequence through S6 layers without gradient checkpointing\"\"\"\n",
        "        for layer in self.s6_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # Process frames\n",
        "        x = x.reshape(B * T, C, H, W)\n",
        "        x = self.conv_encoder(x)\n",
        "\n",
        "        _, C_hidden, H_hidden, W_hidden = x.shape\n",
        "\n",
        "        # Reduce dimensions\n",
        "        x = self.dim_reduce(x)\n",
        "        x = x.view(B, T, -1, H_hidden, W_hidden)\n",
        "        x = x.permute(0, 3, 4, 1, 2)  # [B, H, W, T, C]\n",
        "        x = x.reshape(B * H_hidden * W_hidden, T, -1)\n",
        "\n",
        "        # Process sequence\n",
        "        x = self.process_sequence(x)\n",
        "\n",
        "        # Reshape and expand\n",
        "        x = x.view(B, H_hidden, W_hidden, T, -1)\n",
        "        x = x.permute(0, 3, 4, 1, 2)\n",
        "        x = x.reshape(B * T, -1, H_hidden, W_hidden)\n",
        "        x = self.dim_expand(x)\n",
        "\n",
        "        # Decode\n",
        "        x = self.conv_decoder(x)\n",
        "        x = x.view(B, T, C, H, W)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Updated trainer with proper AMP handling\"\"\"\n",
        "    def __init__(self, model: nn.Module, config: Config):\n",
        "        self.model = model.to(config.device)\n",
        "        self.config = config\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader) -> float:\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(self.config.device)\n",
        "\n",
        "            # Extract frames\n",
        "            start_frames = batch[:, [0]]  # [B, 1, C, H, W]\n",
        "            end_frames = batch[:, [-1]]   # [B, 1, C, H, W]\n",
        "            target_frames = batch[:, 1:-1]  # [B, num_interpolated, C, H, W]\n",
        "\n",
        "            input_frames = torch.cat([start_frames, end_frames], dim=1)\n",
        "\n",
        "            # Use updated autocast\n",
        "            with autocast(device_type='cuda' if self.config.device == 'cuda' else 'cpu'):\n",
        "                pred_frames = self.model(input_frames)\n",
        "                loss = self.criterion(pred_frames, target_frames)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if num_batches % 10 == 0:\n",
        "                logger.info(f\"Batch {num_batches}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        return total_loss / num_batches\n",
        "def generate_dataset(input_folder: str, train_folder: str, test_folder: str, config: Config):\n",
        "    \"\"\"Generate dataset from input videos\"\"\"\n",
        "    processor = FrameProcessor(config.img_size)\n",
        "\n",
        "    for folder in [train_folder, test_folder]:\n",
        "        Path(folder).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    video_files = list(Path(input_folder).glob(\"*.mp4\"))\n",
        "    if not video_files:\n",
        "        raise RuntimeError(f\"No MP4 files found in {input_folder}\")\n",
        "\n",
        "    for video_file in video_files:\n",
        "        logger.info(f\"Processing video: {video_file}\")\n",
        "\n",
        "        frames = processor.extract_frames(str(video_file))\n",
        "        sequence_length = config.num_interpolated + 2\n",
        "\n",
        "        if len(frames) < sequence_length:\n",
        "            logger.warning(f\"Video {video_file} too short, skipping\")\n",
        "            continue\n",
        "\n",
        "        num_sequences = len(frames) - sequence_length + 1\n",
        "        for i in range(num_sequences):\n",
        "            sequence = frames[i:i + sequence_length]\n",
        "\n",
        "            output_dir = Path(train_folder if random.random() < config.train_ratio else test_folder)\n",
        "            seq_folder = output_dir / f\"{video_file.stem}_seq_{i}\"\n",
        "            seq_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "            for j, frame in enumerate(sequence):\n",
        "                frame_path = seq_folder / f\"frame_{j}.jpg\"\n",
        "                cv2.imwrite(str(frame_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        logger.info(f\"Generated {num_sequences} sequences from {video_file}\")\n",
        "\n",
        "class FrameProcessor:\n",
        "    \"\"\"Handles video frame extraction and processing\"\"\"\n",
        "    def __init__(self, img_size: Tuple[int, int]):\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def process_frame(self, frame: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Process a single frame\"\"\"\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, self.img_size)\n",
        "        return frame\n",
        "\n",
        "    def extract_frames(self, video_path: str) -> List[np.ndarray]:\n",
        "        \"\"\"Extracts and processes frames from video\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        try:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = self.process_frame(frame)\n",
        "                frames.append(frame)\n",
        "        finally:\n",
        "            cap.release()\n",
        "        return frames\n",
        "\n",
        "class FrameInterpolationDataset(Dataset):\n",
        "    \"\"\"Dataset class for frame sequences\"\"\"\n",
        "    def __init__(self, root_dir: str, img_size: Tuple[int, int], transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "        self.sequences = list(self.root_dir.glob(\"**/frame_0.jpg\"))\n",
        "\n",
        "        if len(self.sequences) == 0:\n",
        "            raise RuntimeError(f\"No sequences found in {root_dir}\")\n",
        "\n",
        "        logger.info(f\"Found {len(self.sequences)} sequences in {root_dir}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        sequence_dir = self.sequences[idx].parent\n",
        "        frames = []\n",
        "\n",
        "        frame_paths = sorted(sequence_dir.glob(\"*.jpg\"),\n",
        "                           key=lambda x: int(x.stem.split('_')[1]))\n",
        "\n",
        "        for frame_path in frame_paths:\n",
        "            frame = cv2.imread(str(frame_path))\n",
        "            if frame is None:\n",
        "                raise RuntimeError(f\"Failed to load image: {frame_path}\")\n",
        "\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, self.img_size)\n",
        "\n",
        "            frame = torch.from_numpy(frame).float() / 255.0\n",
        "            frame = frame.permute(2, 0, 1)\n",
        "\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "\n",
        "            frames.append(frame)\n",
        "\n",
        "        frames_tensor = torch.stack(frames)\n",
        "\n",
        "        expected_shape = (len(frame_paths), 3, self.img_size[1], self.img_size[0])\n",
        "        if frames_tensor.shape != expected_shape:\n",
        "            raise RuntimeError(\n",
        "                f\"Incorrect tensor shape. Expected {expected_shape}, got {frames_tensor.shape}\"\n",
        "            )\n",
        "\n",
        "        return frames_tensor\n",
        "\n",
        "def main():\n",
        "    # Initialize configuration\n",
        "    config = Config()\n",
        "    logger.info(f\"Using device: {config.device}\")\n",
        "\n",
        "    # Check for input directory and videos\n",
        "    input_dir = Path(\"input\")\n",
        "    if not input_dir.exists() or not list(input_dir.glob(\"*.mp4\")):\n",
        "        logger.error(\"No input directory found or no .mp4 files present\")\n",
        "        return\n",
        "\n",
        "    # Clean existing datasets\n",
        "    for folder in ['train', 'test']:\n",
        "        if Path(folder).exists():\n",
        "            shutil.rmtree(folder)\n",
        "            Path(folder).mkdir(parents=True)\n",
        "\n",
        "    # Generate dataset\n",
        "    generate_dataset(\"input\", \"train\", \"test\", config)\n",
        "\n",
        "    # Verify dataset creation\n",
        "    train_files = list(Path(\"train\").rglob(\"*.jpg\"))\n",
        "    if not train_files:\n",
        "        logger.error(\"No training data generated\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Generated {len(train_files)} training frames\")\n",
        "\n",
        "    # Initialize model and trainer\n",
        "    model = VFIMamba(config)\n",
        "    trainer = Trainer(model, config)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = FrameInterpolationDataset(\n",
        "        \"train\",\n",
        "        img_size=config.img_size,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0 if config.device == \"cpu\" else 4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    logger.info(\"Starting training...\")\n",
        "    for epoch in range(config.epochs):\n",
        "        loss = trainer.train_epoch(train_dataloader)\n",
        "        logger.info(f\"Epoch [{epoch+1}/{config.epochs}] Loss: {loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, checkpoint_path)\n",
        "        logger.info(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), \"frame_interpolation_model.pth\")\n",
        "    logger.info(\"Training completed. Model saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "BeaU10Kt6m4i",
        "outputId": "267d868c-ac19-4c1c-bc3a-b1092c95fb24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BeaU10Kt6m4i",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([5, 1, 3, 128, 128])) that is different to the input size (torch.Size([5, 2, 3, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}